# =====================================================================
# éŸ³é¢‘å¤„ç†æ¨¡å— - Audio Processor Module
# =====================================================================
# åŠŸèƒ½ï¼šè´Ÿè´£æ‰€æœ‰éŸ³é¢‘å½•åˆ¶å’ŒWhisperæ¨¡å‹è¯­éŸ³è¯†åˆ«æ“ä½œ
# ä½œè€…ï¼šNTU EEE MSc Group 2025
# è¯´æ˜ï¼šæ”¯æŒfaster-whisperå’Œopenai-whisperä¸¤ç§åç«¯ï¼Œè‡ªåŠ¨é™çº§
# =====================================================================

import warnings
import os

# ã€é…ç½®ã€‘æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒç»ˆç«¯è¾“å‡ºæ¸…æ™°
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # TensorFlowæ—¥å¿—çº§åˆ«è®¾ä¸ºERROR

import numpy as np
import pyaudio

# ã€å…³é”®ã€‘å°è¯•å¯¼å…¥faster-whisperï¼ˆä¼˜å…ˆï¼‰ï¼Œå¤±è´¥åˆ™é™çº§åˆ°openai-whisper
# faster-whisperä½¿ç”¨CTranslate2å¼•æ“ï¼Œæ”¯æŒint8é‡åŒ–ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«
try:
    from faster_whisper import WhisperModel
    BACKEND = "faster-whisper"
except ImportError:
    print("âš ï¸ faster-whisperä¸å¯ç”¨ï¼Œé™çº§åˆ°openai-whisper")
    import whisper
    BACKEND = "openai-whisper"


class AudioProcessor:
    """
    éŸ³é¢‘å¤„ç†å™¨ç±»
    
    èŒè´£ï¼š
    1. åˆå§‹åŒ–å’Œç®¡ç†Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹
    2. ä»éº¦å…‹é£å½•åˆ¶éŸ³é¢‘æ•°æ®
    3. æ‰§è¡Œå”¤é†’è¯æ£€æµ‹ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
    4. æ‰§è¡Œè¯­éŸ³è½¬æ–‡å­—ï¼ˆæ ‡å‡†æ¨¡å¼ï¼Œæ”¯æŒçŸ­è¯­å¢å¼ºï¼‰
    5. åŒ¹é…è½¬å†™ç»“æœä¸æŒ‡ä»¤åˆ—è¡¨
    
    è®¾è®¡æ¨¡å¼ï¼šç­–ç•¥æ¨¡å¼ï¼ˆæ”¯æŒå¤šç§Whisperåç«¯ï¼‰
    """

    def __init__(self, model_size="base", language="en", device="cpu"):
        """
        åˆå§‹åŒ–Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹
        
        å‚æ•°è¯´æ˜ï¼š
            model_size: æ¨¡å‹å¤§å° - "tiny"(39M)ï¼Œ"base"(74M)ï¼Œ"small"(244M)ï¼Œ
                       "medium"(769M)ï¼Œ"large"(1550M)
                       è¶Šå¤§è¶Šå‡†ç¡®ä½†æ¨ç†è¶Šæ…¢ï¼Œæ¨èbaseå¹³è¡¡æ€§èƒ½
            language: è¯­è¨€ä»£ç  - "en"(è‹±æ–‡)ï¼Œ"zh"(ä¸­æ–‡)ï¼Œæ”¯æŒ99ç§è¯­è¨€
            device: è®¡ç®—è®¾å¤‡ - "cpu"æˆ–"cuda"ï¼Œç¦»çº¿ç³»ç»Ÿæ¨ècpué¿å…GPUä¾èµ–
        
        ã€å…³é”®ã€‘æ¨¡å‹åŠ è½½æµç¨‹ï¼š
        1. é¦–æ¬¡è¿è¡Œä¼šä»HuggingFaceä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆéœ€è”ç½‘ï¼‰
        2. åç»­è¿è¡Œä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼ˆ~/.cache/huggingfaceï¼‰
        3. faster-whisperä½¿ç”¨int8é‡åŒ–ï¼Œå†…å­˜å ç”¨é™ä½75%
        """
        print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–Whisperæ¨¡å‹ (æ¨¡å‹å¤§å°: {model_size})...")

        # ä¿å­˜é…ç½®å‚æ•°
        self.model_size = model_size
        self.language = language
        self.device = device

        # ã€å…³é”®ã€‘æ ¹æ®åç«¯ç±»å‹åŠ è½½æ¨¡å‹
        if BACKEND == "faster-whisper":
            # faster-whisper: CTranslate2å¼•æ“ + int8é‡åŒ–
            self.model = WhisperModel(
                model_size,
                device=device,
                compute_type="int8"  # é‡åŒ–è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†
            )
        else:
            # openai-whisper: åŸå§‹PyTorchå®ç°
            self.model = whisper.load_model(model_size, device=device)

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (åç«¯: {BACKEND})")

    def record_audio(self, duration=5, sample_rate=16000):
        """
        ä»ç³»ç»Ÿéº¦å…‹é£å½•åˆ¶éŸ³é¢‘
        
        å‚æ•°è¯´æ˜ï¼š
            duration: å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå”¤é†’è¯æ£€æµ‹ç”¨3ç§’ï¼ŒæŒ‡ä»¤è¯†åˆ«ç”¨5ç§’
            sample_rate: é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œ16000æ˜¯è¯­éŸ³è¯†åˆ«æ ‡å‡†é‡‡æ ·ç‡
        
        è¿”å›å€¼ï¼š
            numpyæ•°ç»„ï¼Œshape=(sample_count,)ï¼Œæ•°æ®èŒƒå›´[-1.0, 1.0]
            å½•åˆ¶å¤±è´¥è¿”å›None
        
        ã€å…³é”®ã€‘å½•åˆ¶æµç¨‹ï¼š
        1. æ‰“å¼€PyAudioéŸ³é¢‘æµè¿æ¥éº¦å…‹é£
        2. åˆ†å—é‡‡é›†éŸ³é¢‘æ•°æ®ï¼ˆchunk=1024å­—èŠ‚ï¼Œçº¦64mså»¶è¿Ÿï¼‰
        3. é‡‡é›†æŒ‡å®šæ—¶é•¿çš„æ‰€æœ‰æ•°æ®å—
        4. å…³é—­éŸ³é¢‘æµé‡Šæ”¾èµ„æº
        5. å°†åŸå§‹å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºfloat32æ•°ç»„å¹¶å½’ä¸€åŒ–
        
        æ•°æ®æµï¼šéº¦å…‹é£ â†’ PyAudio â†’ bytes â†’ numpy.int16 â†’ numpy.float32
        """
        print(f"ğŸ¤ å¼€å§‹å½•åˆ¶éŸ³é¢‘ï¼Œæ—¶é•¿ {duration} ç§’...")

        # ã€é…ç½®ã€‘PyAudioå‚æ•°è®¾ç½®
        chunk = 1024                    # ç¼“å†²åŒºå¤§å°ï¼Œ1024å­—èŠ‚ = çº¦64ms
        format_type = pyaudio.paInt16   # 16ä½PCMæ ¼å¼
        channels = 1                    # å•å£°é“ï¼ˆè¯­éŸ³è¯†åˆ«æ¨èï¼‰

        try:
            # ã€å…³é”®ã€‘åˆå§‹åŒ–PyAudioå¹¶æ‰“å¼€éŸ³é¢‘æµ
            p = pyaudio.PyAudio()
            stream = p.open(
                format=format_type,
                channels=channels,
                rate=sample_rate,
                input=True,                      # è¾“å…¥æ¨¡å¼ï¼ˆå½•éŸ³ï¼‰
                frames_per_buffer=chunk
            )

            # é‡‡é›†éŸ³é¢‘æ•°æ®å—
            frames = []
            num_chunks = int(sample_rate / chunk * duration)  # è®¡ç®—éœ€è¦çš„å—æ•°
            
            for _ in range(num_chunks):
                data = stream.read(chunk, exception_on_overflow=False)
                frames.append(data)

            # ã€å…³é”®ã€‘æ¸…ç†èµ„æº
            stream.stop_stream()
            stream.close()
            p.terminate()

            print("âœ… å½•åˆ¶å®Œæˆ")

            # ã€æ•°æ®è½¬æ¢ã€‘bytes â†’ numpy.int16 â†’ numpy.float32 (å½’ä¸€åŒ–)
            audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
            audio_data = audio_data.astype(np.float32) / 32768.0  # å½’ä¸€åŒ–åˆ°[-1, 1]

            return audio_data

        except Exception as e:
            print(f"âŒ å½•åˆ¶é”™è¯¯: {e}")
            return None

    def detect_wake_word(self, audio_data, wake_word="susie"):
        """
        æ£€æµ‹éŸ³é¢‘ä¸­æ˜¯å¦åŒ…å«å”¤é†’è¯
        
        å‚æ•°è¯´æ˜ï¼š
            audio_data: å½•åˆ¶çš„éŸ³é¢‘æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
            wake_word: å”¤é†’è¯æ–‡æœ¬ï¼Œé»˜è®¤"susie"
        
        è¿”å›å€¼ï¼š
            True: æ£€æµ‹åˆ°å”¤é†’è¯
            False: æœªæ£€æµ‹åˆ°æˆ–æ£€æµ‹å¤±è´¥
        
        ã€å…³é”®ã€‘æ£€æµ‹ç­–ç•¥ï¼š
        1. ä½¿ç”¨beam_size=1å¿«é€Ÿè½¬å†™æ¨¡å¼ï¼Œé™ä½å»¶è¿Ÿ
        2. å¯ç”¨VAD(Voice Activity Detection)è¿‡æ»¤é™éŸ³æ®µ
        3. è½¬å†™ç»“æœè½¬å°å†™è¿›è¡Œå­å­—ç¬¦ä¸²åŒ¹é…
        4. é€‚ç”¨äºç¬¬ä¸€é˜¶æ®µå¿«é€Ÿå”¤é†’æ£€æµ‹
        
        æ€§èƒ½ä¼˜åŒ–ï¼šbeam_size=1æ¯”beam_size=5å¿«3-4å€
        """
        if audio_data is None:
            return False

        try:
            # ã€å…³é”®ã€‘æ ¹æ®åç«¯æ‰§è¡Œå¿«é€Ÿè½¬å†™
            if BACKEND == "faster-whisper":
                segments, info = self.model.transcribe(
                    audio_data,
                    language=self.language,
                    beam_size=1,        # å¿«é€Ÿæ¨¡å¼ï¼šå•beamæœç´¢
                    vad_filter=True     # VADè¿‡æ»¤ï¼šè·³è¿‡é™éŸ³æ®µ
                )
                # æ‹¼æ¥æ‰€æœ‰segmentçš„æ–‡æœ¬
                text = " ".join([segment.text for segment in segments])
            else:
                # openai-whisperåç«¯
                result = self.model.transcribe(
                    audio_data,
                    language=self.language,
                    fp16=False          # CPUæ¨¡å¼ä¸ä½¿ç”¨åŠç²¾åº¦
                )
                text = result["text"]

            text = text.strip().lower()
            print(f"ğŸ” æ£€æµ‹åˆ°æ–‡æœ¬: {text}")

            # ã€å…³é”®ã€‘å”¤é†’è¯åŒ¹é…é€»è¾‘
            if wake_word.lower() in text:
                print(f"âœ… å”¤é†’è¯ '{wake_word}' å·²æ£€æµ‹åˆ°ï¼")
                return True

            return False

        except Exception as e:
            print(f"âŒ å”¤é†’è¯æ£€æµ‹é”™è¯¯: {e}")
            return False

    def transcribe(self, audio_data, boost_phrases=None):
        """
        å°†éŸ³é¢‘è½¬å†™ä¸ºæ–‡æœ¬ï¼Œæ”¯æŒçŸ­è¯­å¢å¼ºï¼ˆPhrase Boostingï¼‰
        
        å‚æ•°è¯´æ˜ï¼š
            audio_data: å½•åˆ¶çš„éŸ³é¢‘æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
            boost_phrases: çŸ­è¯­åˆ—è¡¨ï¼Œç”¨äºæé«˜ç‰¹å®šçŸ­è¯­çš„è¯†åˆ«å‡†ç¡®åº¦
        
        è¿”å›å€¼ï¼š
            è½¬å†™çš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        
        ã€å…³é”®ã€‘Phrase Boostingæœºåˆ¶ï¼š
        1. é€šè¿‡initial_promptå‚æ•°å°†å·²æ³¨å†ŒæŒ‡ä»¤ä¼ é€’ç»™Whisper
        2. Whisperåœ¨è½¬å†™æ—¶å€¾å‘äºè¯†åˆ«promptä¸­çš„çŸ­è¯­
        3. æ˜¾è‘—æé«˜å·²çŸ¥æŒ‡ä»¤çš„è¯†åˆ«å‡†ç¡®åº¦ï¼ˆç‰¹åˆ«æ˜¯çŸ­æŒ‡ä»¤ï¼‰
        4. å–å‰10ä¸ªæœ€é«˜æƒé‡çš„æŒ‡ä»¤ä½œä¸ºprompt
        
        ã€å…³é”®ã€‘è½¬å†™è´¨é‡ä¼˜åŒ–ï¼š
        - beam_size=5: ä½¿ç”¨5ä¸ªbeamè¿›è¡Œæœç´¢ï¼Œæé«˜å‡†ç¡®åº¦
        - vad_filter=True: è¿‡æ»¤é™éŸ³æ®µï¼Œå‡å°‘è¯¯è¯†åˆ«
        - initial_prompt: çŸ­è¯­å¢å¼ºï¼Œé’ˆå¯¹å·²çŸ¥æŒ‡ä»¤ä¼˜åŒ–
        
        æ•°æ®æµï¼šéŸ³é¢‘ â†’ Whisperæ¨¡å‹ â†’ segments â†’ æ‹¼æ¥æ–‡æœ¬
        """
        if audio_data is None:
            return None

        try:
            # ã€å…³é”®ã€‘å‡†å¤‡çŸ­è¯­å¢å¼ºprompt
            initial_prompt = None
            if boost_phrases and len(boost_phrases) > 0:
                # å–å‰10ä¸ªçŸ­è¯­ï¼Œç”¨é€—å·åˆ†éš”
                initial_prompt = ", ".join(boost_phrases[:10])
                print(f"ğŸ“‹ ä½¿ç”¨çŸ­è¯­å¢å¼º: {initial_prompt}")

            # ã€å…³é”®ã€‘æ ¹æ®åç«¯æ‰§è¡Œé«˜è´¨é‡è½¬å†™
            if BACKEND == "faster-whisper":
                segments, info = self.model.transcribe(
                    audio_data,
                    language=self.language,
                    beam_size=5,              # æ ‡å‡†æ¨¡å¼ï¼š5ä¸ªbeamæœç´¢
                    vad_filter=True,          # VADè¿‡æ»¤
                    initial_prompt=initial_prompt  # çŸ­è¯­å¢å¼º
                )
                # æ‹¼æ¥æ‰€æœ‰segment
                text = " ".join([segment.text for segment in segments])
            else:
                # openai-whisperåç«¯
                result = self.model.transcribe(
                    audio_data,
                    language=self.language,
                    fp16=False,
                    initial_prompt=initial_prompt
                )
                text = result["text"]

            text = text.strip()
            print(f"ğŸ“ è½¬å†™ç»“æœ: {text}")

            return text

        except Exception as e:
            print(f"âŒ è½¬å†™é”™è¯¯: {e}")
            return None

    def check_phrase_match(self, transcribed_text, command_list):
        """
        æ£€æŸ¥è½¬å†™æ–‡æœ¬æ˜¯å¦åŒ¹é…æŒ‡ä»¤åˆ—è¡¨ä¸­çš„ä»»ä¸€æŒ‡ä»¤
        
        å‚æ•°è¯´æ˜ï¼š
            transcribed_text: Whisperè½¬å†™çš„æ–‡æœ¬
            command_list: å·²æ³¨å†Œçš„æŒ‡ä»¤åˆ—è¡¨
        
        è¿”å›å€¼ï¼š
            åŒ¹é…çš„æŒ‡ä»¤æ–‡æœ¬ï¼ŒæœªåŒ¹é…è¿”å›None
        
        ã€å…³é”®ã€‘åŒ¹é…ç­–ç•¥ï¼š
        1. ä¸åŒºåˆ†å¤§å°å†™è¿›è¡Œæ¯”è¾ƒ
        2. æ”¯æŒç²¾ç¡®åŒ¹é…ï¼šè½¬å†™æ–‡æœ¬ == æŒ‡ä»¤
        3. æ”¯æŒå­å­—ç¬¦ä¸²åŒ¹é…ï¼šæŒ‡ä»¤ in è½¬å†™æ–‡æœ¬
        4. é€‚åº”ç”¨æˆ·å¯èƒ½è¯´å‡ºå®Œæ•´å¥å­çš„æƒ…å†µ
        
        ç¤ºä¾‹ï¼š
        - è½¬å†™ï¼š"open door" â†’ åŒ¹é…æŒ‡ä»¤"open door"ï¼ˆç²¾ç¡®ï¼‰
        - è½¬å†™ï¼š"please open door now" â†’ åŒ¹é…æŒ‡ä»¤"open door"ï¼ˆå­ä¸²ï¼‰
        
        æ•°æ®æµï¼šè½¬å†™æ–‡æœ¬ â†’ éå†æŒ‡ä»¤åˆ—è¡¨ â†’ è¿”å›é¦–ä¸ªåŒ¹é…é¡¹
        """
        if not transcribed_text or not command_list:
            return None

        transcribed_lower = transcribed_text.lower().strip()

        # ã€å…³é”®ã€‘éå†æ‰€æœ‰å·²æ³¨å†ŒæŒ‡ä»¤è¿›è¡ŒåŒ¹é…
        for command in command_list:
            command_lower = command.lower().strip()

            # ç²¾ç¡®åŒ¹é…æˆ–å­å­—ç¬¦ä¸²åŒ¹é…
            if command_lower == transcribed_lower or command_lower in transcribed_lower:
                print(f"âœ… åŒ¹é…æˆåŠŸ: '{command}'")
                return command

        print(f"âŒ æœªåŒ¹é…: '{transcribed_text}'")
        return None