# =====================================================================
# æ¨¡å‹è®­ç»ƒæ¨¡å— - Model Training Module
# =====================================================================
# åŠŸèƒ½ï¼šç®¡ç†Whisperæ¨¡å‹çš„è®­ç»ƒæ•°æ®é‡‡é›†ã€æ ‡æ³¨ã€è®­ç»ƒå’Œè¯„ä¼°
# ä½œè€…ï¼šNTU EEE MSc Group 2025
# è¯´æ˜ï¼šæ”¯æŒéŸ³é¢‘æ ·æœ¬æ”¶é›†ã€æ•°æ®é›†ç®¡ç†ã€æ¨¡å‹å¾®è°ƒå’Œæ£€æŸ¥ç‚¹ç®¡ç†
# =====================================================================

import os
import json
import shutil
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import hashlib


class TrainingDataManager:
    """
    è®­ç»ƒæ•°æ®ç®¡ç†å™¨ç±»
    
    èŒè´£ï¼š
    1. é‡‡é›†å’Œå­˜å‚¨éŸ³é¢‘æ ·æœ¬
    2. æ ‡æ³¨å’ŒéªŒè¯è®­ç»ƒæ ·æœ¬
    3. ç®¡ç†æ•°æ®é›†åˆ†å‰²ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
    4. ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
    5. è®°å½•æ•°æ®é›†å…ƒä¿¡æ¯
    
    æ•°æ®ç›®å½•ç»“æ„ï¼š
    training_data/
    â”œâ”€â”€ samples/           # åŸå§‹éŸ³é¢‘æ ·æœ¬
    â”‚   â”œâ”€â”€ sample_001.wav
    â”‚   â”œâ”€â”€ sample_002.wav
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ labels.json        # æ ·æœ¬æ ‡æ³¨ä¿¡æ¯
    â”œâ”€â”€ dataset.json       # æ•°æ®é›†åˆ†å‰²ä¿¡æ¯
    â””â”€â”€ metadata.json      # æ•°æ®é›†å…ƒä¿¡æ¯
    """

    def __init__(self, data_dir="training_data"):
        """
        åˆå§‹åŒ–è®­ç»ƒæ•°æ®ç®¡ç†å™¨
        
        å‚æ•°è¯´æ˜ï¼š
            data_dir: è®­ç»ƒæ•°æ®æ ¹ç›®å½•
        
        ã€å…³é”®ã€‘ç›®å½•ç»“æ„åˆå§‹åŒ–ï¼š
        - ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿Windowså…¼å®¹æ€§
        - è‡ªåŠ¨åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        - åŠ è½½å·²æœ‰çš„æ ‡æ³¨å’Œæ•°æ®é›†ä¿¡æ¯
        """
        self.data_dir = os.path.abspath(data_dir)
        self.samples_dir = os.path.join(self.data_dir, "samples")
        self.labels_file = os.path.join(self.data_dir, "labels.json")
        self.dataset_file = os.path.join(self.data_dir, "dataset.json")
        self.metadata_file = os.path.join(self.data_dir, "metadata.json")
        
        # ã€å…³é”®ã€‘åˆ›å»ºç›®å½•ç»“æ„
        os.makedirs(self.samples_dir, exist_ok=True)
        
        # ã€å…³é”®ã€‘åŠ è½½å·²æœ‰æ•°æ®
        self.labels = self._load_labels()
        self.dataset = self._load_dataset()
        self.metadata = self._load_metadata()

    def _load_labels(self) -> Dict:
        """
        åŠ è½½æ ·æœ¬æ ‡æ³¨ä¿¡æ¯
        
        è¿”å›å€¼ï¼š
            æ ‡æ³¨å­—å…¸ {sample_id: {text, speaker_id, timestamp, verified}}
        """
        if os.path.exists(self.labels_file):
            try:
                with open(self.labels_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ ‡æ³¨æ–‡ä»¶é”™è¯¯: {e}")
                return {}
        return {}

    def _save_labels(self):
        """ä¿å­˜æ ·æœ¬æ ‡æ³¨ä¿¡æ¯"""
        try:
            with open(self.labels_file, 'w', encoding='utf-8') as f:
                json.dump(self.labels, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ ‡æ³¨æ–‡ä»¶é”™è¯¯: {e}")

    def _load_dataset(self) -> Dict:
        """
        åŠ è½½æ•°æ®é›†åˆ†å‰²ä¿¡æ¯
        
        è¿”å›å€¼ï¼š
            æ•°æ®é›†å­—å…¸ {train: [...], val: [...], test: [...]}
        """
        if os.path.exists(self.dataset_file):
            try:
                with open(self.dataset_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ•°æ®é›†æ–‡ä»¶é”™è¯¯: {e}")
                return {"train": [], "val": [], "test": []}
        return {"train": [], "val": [], "test": []}

    def _save_dataset(self):
        """ä¿å­˜æ•°æ®é›†åˆ†å‰²ä¿¡æ¯"""
        try:
            with open(self.dataset_file, 'w', encoding='utf-8') as f:
                json.dump(self.dataset, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®é›†æ–‡ä»¶é”™è¯¯: {e}")

    def _load_metadata(self) -> Dict:
        """
        åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯
        
        è¿”å›å€¼ï¼š
            å…ƒä¿¡æ¯å­—å…¸ {created_at, total_samples, last_updated, ...}
        """
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å…ƒä¿¡æ¯æ–‡ä»¶é”™è¯¯: {e}")
                return self._create_default_metadata()
        return self._create_default_metadata()

    def _create_default_metadata(self) -> Dict:
        """åˆ›å»ºé»˜è®¤å…ƒä¿¡æ¯"""
        return {
            "created_at": datetime.now().isoformat(),
            "total_samples": 0,
            "labeled_samples": 0,
            "verified_samples": 0,
            "last_updated": datetime.now().isoformat(),
            "version": "1.0"
        }

    def _save_metadata(self):
        """ä¿å­˜æ•°æ®é›†å…ƒä¿¡æ¯"""
        self.metadata["last_updated"] = datetime.now().isoformat()
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜å…ƒä¿¡æ¯æ–‡ä»¶é”™è¯¯: {e}")

    def add_sample(self, audio_data, text: str, speaker_id: Optional[str] = None) -> str:
        """
        æ·»åŠ éŸ³é¢‘æ ·æœ¬åˆ°è®­ç»ƒæ•°æ®é›†
        
        å‚æ•°è¯´æ˜ï¼š
            audio_data: éŸ³é¢‘æ•°æ®ï¼ˆnumpyæ•°ç»„æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
            text: æ ·æœ¬çš„æ–‡æœ¬æ ‡æ³¨
            speaker_id: å¯é€‰ï¼Œè¯´è¯äººID
        
        è¿”å›å€¼ï¼š
            ç”Ÿæˆçš„æ ·æœ¬ID
        
        ã€å…³é”®ã€‘æ ·æœ¬å¤„ç†æµç¨‹ï¼š
        1. ç”Ÿæˆå”¯ä¸€æ ·æœ¬IDï¼ˆåŸºäºæ—¶é—´æˆ³+å“ˆå¸Œï¼‰
        2. ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°samplesç›®å½•
        3. åˆ›å»ºæ ‡æ³¨è®°å½•
        4. æ›´æ–°å…ƒä¿¡æ¯
        
        ã€æ•°æ®æµã€‘éŸ³é¢‘æ•°æ® â†’ ä¿å­˜æ–‡ä»¶ â†’ åˆ›å»ºæ ‡æ³¨ â†’ æ›´æ–°å…ƒæ•°æ®
        """
        # ã€å…³é”®ã€‘ç”Ÿæˆå”¯ä¸€æ ·æœ¬ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hash_suffix = hashlib.md5(text.encode()).hexdigest()[:6]
        sample_id = f"sample_{timestamp}_{hash_suffix}"
        
        # ã€å…³é”®ã€‘ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆå®é™…å®ç°éœ€è¦éŸ³é¢‘ç¼–ç ï¼‰
        # è¿™é‡Œå‡è®¾audio_dataæ˜¯numpyæ•°ç»„ï¼Œå®é™…éœ€è¦è½¬ä¸ºWAVæ ¼å¼
        # ç®€åŒ–å®ç°ï¼šå¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥å¤åˆ¶
        if isinstance(audio_data, str) and os.path.exists(audio_data):
            sample_path = os.path.join(self.samples_dir, f"{sample_id}.wav")
            shutil.copy(audio_data, sample_path)
        else:
            # å®é™…åº”ç”¨éœ€è¦å°†numpyæ•°ç»„ä¿å­˜ä¸ºWAV
            sample_path = os.path.join(self.samples_dir, f"{sample_id}.wav")
            # TODO: å®ç°numpyæ•°ç»„åˆ°WAVçš„è½¬æ¢
            print(f"âš ï¸ éŸ³é¢‘æ•°æ®ä¿å­˜åŠŸèƒ½éœ€è¦å®ç°")
        
        # ã€å…³é”®ã€‘åˆ›å»ºæ ‡æ³¨è®°å½•
        self.labels[sample_id] = {
            "text": text,
            "speaker_id": speaker_id,
            "timestamp": datetime.now().isoformat(),
            "verified": False,
            "file_path": sample_path
        }
        
        # ã€å…³é”®ã€‘æ›´æ–°å…ƒä¿¡æ¯
        self.metadata["total_samples"] += 1
        self.metadata["labeled_samples"] += 1
        
        # ä¿å­˜åˆ°ç£ç›˜
        self._save_labels()
        self._save_metadata()
        
        print(f"âœ… å·²æ·»åŠ æ ·æœ¬: {sample_id}")
        return sample_id

    def verify_sample(self, sample_id: str, verified: bool = True):
        """
        éªŒè¯æ ·æœ¬æ ‡æ³¨çš„å‡†ç¡®æ€§
        
        å‚æ•°è¯´æ˜ï¼š
            sample_id: æ ·æœ¬ID
            verified: æ˜¯å¦éªŒè¯é€šè¿‡
        
        ã€å…³é”®ã€‘éªŒè¯æµç¨‹ï¼š
        - æ ‡è®°æ ·æœ¬ä¸ºå·²éªŒè¯
        - æ›´æ–°verified_samplesè®¡æ•°
        """
        if sample_id in self.labels:
            self.labels[sample_id]["verified"] = verified
            if verified:
                self.metadata["verified_samples"] = sum(
                    1 for label in self.labels.values() if label.get("verified", False)
                )
            self._save_labels()
            self._save_metadata()
            print(f"âœ… æ ·æœ¬éªŒè¯çŠ¶æ€å·²æ›´æ–°: {sample_id} -> {verified}")
        else:
            print(f"âŒ æ ·æœ¬ä¸å­˜åœ¨: {sample_id}")

    def split_dataset(self, train_ratio: float = 0.7, val_ratio: float = 0.15, 
                     test_ratio: float = 0.15, random_seed: int = 42):
        """
        å°†æ ·æœ¬åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        å‚æ•°è¯´æ˜ï¼š
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤70%ï¼‰
            val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤15%ï¼‰
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤15%ï¼‰
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
        
        ã€å…³é”®ã€‘åˆ†å‰²ç­–ç•¥ï¼š
        1. ä»…å¯¹å·²éªŒè¯çš„æ ·æœ¬è¿›è¡Œåˆ†å‰²
        2. ä½¿ç”¨éšæœºç§å­ç¡®ä¿åˆ†å‰²å¯å¤ç°
        3. æŒ‰ç…§æŒ‡å®šæ¯”ä¾‹åˆ†é…æ ·æœ¬
        4. ä¿å­˜åˆ†å‰²ç»“æœåˆ°dataset.json
        
        ã€æ•°æ®æµã€‘å·²éªŒè¯æ ·æœ¬ â†’ éšæœºæ‰“ä¹± â†’ æŒ‰æ¯”ä¾‹åˆ†å‰² â†’ ä¿å­˜ç»“æœ
        """
        # ã€å…³é”®ã€‘è·å–æ‰€æœ‰å·²éªŒè¯çš„æ ·æœ¬
        verified_samples = [
            sample_id for sample_id, label in self.labels.items()
            if label.get("verified", False)
        ]
        
        if len(verified_samples) == 0:
            print("âš ï¸ æ²¡æœ‰å·²éªŒè¯çš„æ ·æœ¬å¯ä¾›åˆ†å‰²")
            return
        
        # ã€å…³é”®ã€‘éšæœºæ‰“ä¹±æ ·æœ¬ï¼ˆä½¿ç”¨å›ºå®šç§å­ï¼‰
        import random
        random.seed(random_seed)
        random.shuffle(verified_samples)
        
        # ã€å…³é”®ã€‘è®¡ç®—åˆ†å‰²ç‚¹
        total = len(verified_samples)
        train_end = int(total * train_ratio)
        val_end = train_end + int(total * val_ratio)
        
        # ã€å…³é”®ã€‘åˆ†å‰²æ ·æœ¬
        self.dataset = {
            "train": verified_samples[:train_end],
            "val": verified_samples[train_end:val_end],
            "test": verified_samples[val_end:]
        }
        
        # ä¿å­˜åˆ†å‰²ç»“æœ
        self._save_dataset()
        
        print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(self.dataset['train'])} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(self.dataset['val'])} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(self.dataset['test'])} æ ·æœ¬")

    def get_batch(self, split: str, batch_size: int = 32) -> List[Dict]:
        """
        è·å–æŒ‡å®šæ•°æ®é›†çš„æ‰¹æ¬¡æ•°æ®
        
        å‚æ•°è¯´æ˜ï¼š
            split: æ•°æ®é›†ç±»å‹ ("train", "val", "test")
            batch_size: æ‰¹æ¬¡å¤§å°
        
        è¿”å›å€¼ï¼š
            æ‰¹æ¬¡æ ·æœ¬åˆ—è¡¨ [{sample_id, text, audio_path, speaker_id}, ...]
        
        ã€ç”¨é€”ã€‘ä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ‰¹æ¬¡æ•°æ®
        """
        if split not in self.dataset:
            print(f"âŒ æ— æ•ˆçš„æ•°æ®é›†ç±»å‹: {split}")
            return []
        
        sample_ids = self.dataset[split]
        batch = []
        
        for sample_id in sample_ids[:batch_size]:
            if sample_id in self.labels:
                label = self.labels[sample_id]
                batch.append({
                    "sample_id": sample_id,
                    "text": label["text"],
                    "audio_path": label["file_path"],
                    "speaker_id": label.get("speaker_id")
                })
        
        return batch

    def get_statistics(self) -> Dict:
        """
        è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›å€¼ï¼š
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "total_samples": self.metadata["total_samples"],
            "labeled_samples": self.metadata["labeled_samples"],
            "verified_samples": self.metadata["verified_samples"],
            "train_samples": len(self.dataset.get("train", [])),
            "val_samples": len(self.dataset.get("val", [])),
            "test_samples": len(self.dataset.get("test", [])),
            "speakers": len(set(
                label.get("speaker_id") for label in self.labels.values()
                if label.get("speaker_id")
            ))
        }
        return stats


class ModelTrainer:
    """
    æ¨¡å‹è®­ç»ƒå™¨ç±»
    
    èŒè´£ï¼š
    1. ç®¡ç†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹
    2. æ‰§è¡Œå¾®è°ƒå’Œå‚æ•°ä¼˜åŒ–
    3. è¯„ä¼°æ¨¡å‹å‡†ç¡®åº¦
    4. ç®¡ç†è®­ç»ƒæ£€æŸ¥ç‚¹
    5. è®°å½•è®­ç»ƒæ—¥å¿—
    
    ã€æ³¨æ„ã€‘å®é™…çš„Whisperæ¨¡å‹å¾®è°ƒéœ€è¦ï¼š
    - å¤§é‡GPUè®¡ç®—èµ„æº
    - HuggingFace transformersåº“
    - ä¸“é—¨çš„è®­ç»ƒé…ç½®
    
    æœ¬å®ç°æä¾›è®­ç»ƒæ¡†æ¶ï¼Œå¯æ‰©å±•ä¸ºå®é™…è®­ç»ƒåŠŸèƒ½
    """

    def __init__(self, model_name: str = "base", checkpoint_dir: str = "checkpoints"):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        å‚æ•°è¯´æ˜ï¼š
            model_name: Whisperæ¨¡å‹åç§°
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        """
        self.model_name = model_name
        self.checkpoint_dir = os.path.abspath(checkpoint_dir)
        self.training_log_file = os.path.join(self.checkpoint_dir, "training_log.json")
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # åŠ è½½è®­ç»ƒæ—¥å¿—
        self.training_log = self._load_training_log()

    def _load_training_log(self) -> List[Dict]:
        """åŠ è½½è®­ç»ƒæ—¥å¿—"""
        if os.path.exists(self.training_log_file):
            try:
                with open(self.training_log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è®­ç»ƒæ—¥å¿—é”™è¯¯: {e}")
                return []
        return []

    def _save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        try:
            with open(self.training_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_log, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜è®­ç»ƒæ—¥å¿—é”™è¯¯: {e}")

    def train(self, data_manager: TrainingDataManager, epochs: int = 10, 
              batch_size: int = 16, learning_rate: float = 1e-5) -> Dict:
        """
        æ‰§è¡Œæ¨¡å‹è®­ç»ƒ
        
        å‚æ•°è¯´æ˜ï¼š
            data_manager: è®­ç»ƒæ•°æ®ç®¡ç†å™¨
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
        
        è¿”å›å€¼ï¼š
            è®­ç»ƒç»“æœå­—å…¸ {loss, accuracy, checkpoint_path}
        
        ã€å…³é”®ã€‘è®­ç»ƒæµç¨‹ï¼š
        1. éªŒè¯æ•°æ®é›†æ˜¯å¦å°±ç»ª
        2. åˆå§‹åŒ–æ¨¡å‹ï¼ˆæˆ–åŠ è½½æ£€æŸ¥ç‚¹ï¼‰
        3. è¿­ä»£è®­ç»ƒepochsè½®
        4. æ¯è½®è¯„ä¼°éªŒè¯é›†æ€§èƒ½
        5. ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        6. è®°å½•è®­ç»ƒæ—¥å¿—
        
        ã€æ³¨æ„ã€‘å®é™…å®ç°éœ€è¦ï¼š
        - åŠ è½½Whisperæ¨¡å‹
        - å®ç°å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
        - ä¼˜åŒ–å™¨é…ç½®
        - æŸå¤±å‡½æ•°è®¡ç®—
        """
        print(f"ğŸ”„ å¼€å§‹è®­ç»ƒæ¨¡å‹: {self.model_name}")
        print(f"   å‚æ•°: epochs={epochs}, batch_size={batch_size}, lr={learning_rate}")
        
        # ã€å…³é”®ã€‘éªŒè¯æ•°æ®é›†
        stats = data_manager.get_statistics()
        if stats["train_samples"] == 0:
            print("âŒ è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ")
            return {"error": "No training data"}
        
        # ã€å…³é”®ã€‘æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆå®é™…éœ€è¦å®ç°çœŸå®è®­ç»ƒï¼‰
        print("âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿè®­ç»ƒï¼Œå®é™…è®­ç»ƒéœ€è¦GPUå’Œtransformersåº“")
        
        # è®­ç»ƒæ—¥å¿—æ¡ç›®
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model_name": self.model_name,
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "train_samples": stats["train_samples"],
            "val_samples": stats["val_samples"],
            "status": "completed",
            "final_loss": 0.0,  # æ¨¡æ‹Ÿå€¼
            "val_accuracy": 0.95,  # æ¨¡æ‹Ÿå€¼
            "checkpoint_path": os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epochs}.pt")
        }
        
        # ä¿å­˜æ—¥å¿—
        self.training_log.append(log_entry)
        self._save_training_log()
        
        print(f"âœ… è®­ç»ƒå®Œæˆ (æ¨¡æ‹Ÿ)")
        print(f"   éªŒè¯é›†å‡†ç¡®åº¦: {log_entry['val_accuracy']:.2%}")
        print(f"   æ£€æŸ¥ç‚¹: {log_entry['checkpoint_path']}")
        
        return log_entry

    def evaluate(self, data_manager: TrainingDataManager, checkpoint_path: Optional[str] = None) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        å‚æ•°è¯´æ˜ï¼š
            data_manager: è®­ç»ƒæ•°æ®ç®¡ç†å™¨
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›å€¼ï¼š
            è¯„ä¼°ç»“æœå­—å…¸ {accuracy, wer, precision, recall}
        
        ã€å…³é”®ã€‘è¯„ä¼°æŒ‡æ ‡ï¼š
        - Accuracy: æ•´ä½“å‡†ç¡®ç‡
        - WER (Word Error Rate): è¯é”™è¯¯ç‡
        - Precision: ç²¾ç¡®ç‡
        - Recall: å¬å›ç‡
        """
        print(f"ğŸ”„ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # ã€å…³é”®ã€‘è·å–æµ‹è¯•é›†æ•°æ®
        test_samples = data_manager.get_batch("test", batch_size=100)
        
        if len(test_samples) == 0:
            print("âš ï¸ æµ‹è¯•é›†ä¸ºç©º")
            return {"error": "No test data"}
        
        # ã€å…³é”®ã€‘æ¨¡æ‹Ÿè¯„ä¼°ï¼ˆå®é™…éœ€è¦åŠ è½½æ¨¡å‹å¹¶æ¨ç†ï¼‰
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_samples": len(test_samples),
            "accuracy": 0.92,  # æ¨¡æ‹Ÿå€¼
            "wer": 0.08,  # æ¨¡æ‹Ÿå€¼
            "precision": 0.93,  # æ¨¡æ‹Ÿå€¼
            "recall": 0.91,  # æ¨¡æ‹Ÿå€¼
            "checkpoint_path": checkpoint_path
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆ:")
        print(f"   å‡†ç¡®ç‡: {results['accuracy']:.2%}")
        print(f"   è¯é”™è¯¯ç‡: {results['wer']:.2%}")
        
        return results

    def save_checkpoint(self, model_state: Dict, optimizer_state: Dict, 
                       epoch: int, metrics: Dict) -> str:
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
        
        å‚æ•°è¯´æ˜ï¼š
            model_state: æ¨¡å‹çŠ¶æ€å­—å…¸
            optimizer_state: ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸
            epoch: å½“å‰è½®æ•°
            metrics: æ€§èƒ½æŒ‡æ ‡
        
        è¿”å›å€¼ï¼š
            æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        
        ã€å…³é”®ã€‘æ£€æŸ¥ç‚¹å†…å®¹ï¼š
        - æ¨¡å‹å‚æ•°
        - ä¼˜åŒ–å™¨çŠ¶æ€
        - è®­ç»ƒè½®æ•°
        - æ€§èƒ½æŒ‡æ ‡
        - æ—¶é—´æˆ³
        """
        checkpoint_name = f"checkpoint_epoch_{epoch}.json"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        
        checkpoint = {
            "model_name": self.model_name,
            "epoch": epoch,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "model_state": "saved",  # å®é™…åº”ä¿å­˜æ¨¡å‹æƒé‡
            "optimizer_state": "saved"  # å®é™…åº”ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        }
        
        try:
            with open(checkpoint_path, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, indent=2, ensure_ascii=False)
            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
            return checkpoint_path
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹é”™è¯¯: {e}")
            return ""

    def load_checkpoint(self, checkpoint_path: str) -> Optional[Dict]:
        """
        åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹
        
        å‚æ•°è¯´æ˜ï¼š
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        
        è¿”å›å€¼ï¼š
            æ£€æŸ¥ç‚¹æ•°æ®å­—å…¸ï¼ŒåŠ è½½å¤±è´¥è¿”å›None
        """
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)
            print(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
            return checkpoint
        except Exception as e:
            print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹é”™è¯¯: {e}")
            return None

    def get_training_history(self) -> List[Dict]:
        """
        è·å–è®­ç»ƒå†å²è®°å½•
        
        è¿”å›å€¼ï¼š
            è®­ç»ƒæ—¥å¿—åˆ—è¡¨
        """
        return self.training_log
